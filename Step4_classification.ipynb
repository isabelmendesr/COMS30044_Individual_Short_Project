{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7l1ZVgY3Cxwg"},"outputs":[],"source":["from my_utils import emotion_dict, sentiment_dict, film_frames_dict\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.pipeline import make_pipeline, Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PY9lni8jD9U8"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","folder = '/content/drive/My Drive/Colab Notebooks/Dissertation'\n","\n","os.chdir(folder)\n","\n","output_dir = './Datasets'\n","results_dir = './Results'\n","\n","film_keys = list(emotion_dict.keys())\n","datasets = ['frame_level', 'video_level']\n","results = ['eda', 'stats_analysis','modelling']\n","modelling_folders = ['classification_results','plots']\n","feature_group = ['rgb_hsv','audio', 'optical_flow']\n","\n","for dataset in datasets:\n","    os.makedirs(os.path.join(results_dir, dataset), exist_ok=True)\n","    for results_type in results:\n","        os.makedirs(os.path.join(results_dir, dataset, results_type), exist_ok=True)\n","        if results_type == 'modelling':\n","            for folder in modelling_folders:\n","                os.makedirs(os.path.join(results_dir, dataset, results_type, folder), exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"ob7Cdl05VjSJ"},"source":["#### Function to load feature sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pWELJUIGVjSK"},"outputs":[],"source":["# obtain the frame-level and video-level feature sets\n","def load_datasets(frame_level_path, video_level_path):\n","    if os.path.exists(frame_level_path):\n","        frame_df = pd.read_csv(frame_level_path)\n","    else:\n","        frame_df = None\n","        print (f\"File {frame_level_path} not found\")\n","\n","    if os.path.exists(video_level_path):\n","        video_df = pd.read_csv(video_level_path)\n","    else:\n","        video_df = None\n","        print (f\"File {video_level_path} not found\")\n","\n","    return frame_df, video_df\n","\n","# obtain the frame-level and video-level feature sets based on the feature group (rgb_hsv, audio, optical_flow)\n","\n","def get_feature_group_datasets(feature_group):\n","    # call load_datasets above\n","    frame_df, video_df = load_datasets(os.path.join(output_dir, f\"{datasets[0]}/features_{feature_group}_df.csv\"),\n","                                       os.path.join(output_dir, f\"{datasets[1]}/features_{feature_group}_df.csv\"))\n","\n","    v_shape = ()\n","    f_shape = ()\n","    feature_cols = []\n","\n","    if video_df is not None:\n","        feature_cols = [col for col in video_df.columns if col not in [\"video_id\", \"emotion\", \"sentiment\"]]\n","        v_shape = video_df.shape\n","\n","    if frame_df is not None:\n","        f_shape = frame_df.shape\n","\n","        feature_cols = [col for col in frame_df.columns if col not in [\"video_id\", \"frame_id\", \"emotion\", \"sentiment\"]]\n","\n","    print(f\"Dataframes shape for {feature_group}: frame-level -> {f_shape}, video-level -> {v_shape}\")\n","\n","    return frame_df, video_df, feature_cols\n"]},{"cell_type":"markdown","metadata":{"id":"udSUQYQpVjSL"},"source":["#### Datasets concatenation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0ENtczzVjSM"},"outputs":[],"source":["# combine different subsets of features by joining different data frames\n","def join_feature_datasets(feature_group_list):\n","\n","    # initialise merged_df as None\n","    v_merged_df = None\n","    f_merged_df = None\n","\n","    v_flag = True\n","    f_flag = True\n","\n","    merged_feature_group = \"_\".join([feature_group for feature_group in feature_group_list])\n","\n","    for feature_group in feature_group_list:\n","        frame_df, video_df = load_datasets(os.path.join(output_dir, f\"{datasets[0]}/features_{feature_group}_df.csv\"),\n","                                           os.path.join(output_dir, f\"{datasets[1]}/features_{feature_group}_df.csv\"))\n","\n","        if video_df is not None: # joining video_level sets\n","            col_id = [\"video_id\", \"emotion\", \"sentiment\"]\n","\n","            # join based on column ids\n","            if v_merged_df is None:\n","                v_merged_df = video_df\n","            else:\n","                v_merged_df = pd.merge(v_merged_df, video_df, on=col_id, how=\"inner\")\n","        else:\n","            v_flag = False\n","            print(f\"Dataframe for {feature_group} is empty.\")\n","\n","        if frame_df is not None:  # joining frame_level sets\n","            col_id = [\"video_id\", \"frame_id\", \"emotion\", \"sentiment\"]\n","\n","            # join based on column ids\n","            if f_merged_df is None:\n","                f_merged_df = frame_df\n","            else:\n","                f_merged_df = pd.merge(f_merged_df, frame_df, on=col_id, how=\"inner\")\n","\n","        else:\n","            f_flag = False\n","            print(f\"Dataframe for {feature_group} is empty.\")\n","\n","\n","    if v_flag == True:\n","        file_path0 = os.path.join(output_dir, f\"{datasets[0]}/features_{merged_feature_group}_df.csv\")\n","        v_merged_df.to_csv(file_path0, index=False)\n","        print(f\"Merged dataset saved at: {file_path0}\")\n","\n","    if f_flag == True:\n","        file_path1 = os.path.join(output_dir, f\"{datasets[1]}/features_{merged_feature_group}_df.csv\")\n","        f_merged_df.to_csv(file_path1, index=False)\n","        print(f\"Merged dataset saved at: {file_path1}\")\n","\n","    return merged_feature_group"]},{"cell_type":"markdown","metadata":{"id":"RYFTSu76VjSN"},"source":["# 3. Modelling"]},{"cell_type":"markdown","metadata":{"id":"3HtKIPDTVjSN"},"source":["#### Prepare dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zsZnHbrtVjSO"},"outputs":[],"source":["def get_training_data(df, feature_cols, target_col):\n","\n","    X = df[feature_cols]\n","    y = df[target_col]\n","\n","    # encode emotion labels to numeric values\n","    le = LabelEncoder()\n","    y_encoded = le.fit_transform(y)\n","\n","    # split the data (stratified by the training label to maintain balanced classes)\n","    test_size = 0.20  # for 80-20 split\n","    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=test_size, random_state=24, stratify=y_encoded)\n","\n","    return X_train, X_test, y_train, y_test, le"]},{"cell_type":"markdown","metadata":{"id":"ao_fufIAVjSP"},"source":["#### Classification pipelines and hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnWJgq3HVjSP"},"outputs":[],"source":["# defining classifier parameter grids for hyperparameter tuning\n","\n","pipelines = {\n","    'Logistic Regression': Pipeline([\n","        ('scaler', StandardScaler()),\n","        ('clf', LogisticRegression(max_iter=3000, random_state=0))\n","    ]),\n","    'Random Forest': Pipeline([\n","        ('scaler', StandardScaler()),\n","        ('clf', RandomForestClassifier(random_state=0))\n","    ]),\n","    'K-Nearest Neighbors': Pipeline([\n","        ('scaler', StandardScaler()),\n","        ('clf', KNeighborsClassifier())\n","    ]),\n","    'Decision Tree': Pipeline([\n","        ('scaler', StandardScaler()),\n","        ('clf', DecisionTreeClassifier(random_state=0))\n","    ]),\n","    'SVM': Pipeline([\n","        ('scaler', StandardScaler()),\n","        ('clf', SVC(random_state=0))\n","    ])\n","}\n","\n","param_grids = {\n","    'Logistic Regression': {\n","        'clf__C': [0.01, 0.1, 1, 10],\n","        'clf__penalty': ['l2'],\n","        'clf__solver': ['lbfgs', 'saga']\n","    },\n","    'Random Forest': {\n","        'clf__n_estimators': [50, 100, 200],\n","        'clf__max_depth': [None, 5, 10, 20],\n","    },\n","    'K-Nearest Neighbors': {\n","        'clf__n_neighbors': [3, 5, 7, 9, 11]\n","    },\n","    'Decision Tree': {\n","        'clf__max_depth': [None, 5, 10, 20],\n","        'clf__criterion': ['gini', 'entropy']\n","    },\n","    'SVM': {\n","        'clf__C': [0.1, 1, 10],\n","        'clf__gamma': ['scale', 'auto'],\n","        'clf__kernel': ['rbf', 'linear']\n","    }\n","}"]},{"cell_type":"markdown","metadata":{"id":"hLcxu_iVVjSQ"},"source":["#### Functions to Plot Confusion Matrix & Feature Importances"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMIR6E0aVjSQ"},"outputs":[],"source":["# generate confusion matrix for evaluating model performance\n","def plot_confusion_matrix(name, label, label_encoder, y_test, y_pred, feature_group, results_dir=results_dir):\n","    filename = f\"{name}_{label}_{feature_group}_confusion_matrix.png\"\n","    results_dir = os.path.join(results_dir, 'plots', filename)\n","\n","    # plot confusion matrix\n","    cm = confusion_matrix(y_test, y_pred)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n","    plt.xlabel(\"Predicted Label\")\n","    plt.ylabel(\"True Label\")\n","    plt.title(f\"Confusion Matrix: {name} ({feature_group})\")\n","    plt.savefig(os.path.join(results_dir), bbox_inches='tight')\n","    plt.show()\n","\n","\n","# generate feature importances to visualise most predictive features\n","def plot_features_importance(name, clf, label, feature_names, feature_group, results_dir=results_dir, top_n=10):\n","\n","    importance_df = None\n","\n","    filename = f\"{name}_{label}_{feature_group}_feature_importances.png\"\n","    results_dir = os.path.join(results_dir, 'plots', filename)\n","\n","    if hasattr(clf, \"feature_importances_\"): # plot feature importances if these are provided by the chosen classifier\n","        importances = clf.feature_importances_\n","\n","        # create a dataframe to display and sort feature importances\n","        importance_df = pd.DataFrame({\n","            'Feature': feature_names,\n","            'Importance': importances\n","        }).sort_values(by='Importance', ascending=False)\n","\n","        # Print the top 10 important features\n","        print(\"Top 10 Important Features:\")\n","        print(importance_df.head(top_n))\n","\n","        if top_n is not None or top_n < importance_df.shape[0]:\n","            plot_data = importance_df.head(top_n)\n","        else:\n","            plot_data = importance_df\n","\n","        plt.figure(figsize=(8, 6))\n","        sns.barplot(x='Importance', y='Feature', data=plot_data, hue='Feature', palette='viridis')\n","        plt.title(f\"Feature Importances: {name} ({feature_group})\")\n","        plt.xlabel(\"Importance Score\")\n","        plt.ylabel(\"Feature\")\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(results_dir), bbox_inches='tight')\n","        plt.show()\n","\n","    elif hasattr(clf, \"coef_\"): # for Logistic Regression / SVM which output coifficients instead of feature importances\n","        coefs = clf.coef_\n","        importances = np.mean(np.abs(coefs), axis=0) # calculate avg coeff across all emotion classes\n","\n","        # create a dataframe to display and sort feature importances\n","        importance_df = pd.DataFrame({\n","            'Feature': feature_names,\n","            'Importance': importances\n","        }).sort_values(by='Importance', ascending=False)\n","\n","        # Print the top 10 important features\n","        print(\"Top 10 Important Features:\")\n","        print(importance_df.head(top_n))\n","\n","        plt.figure(figsize=(8, 6))\n","        sns.barplot(x='Importance', y='Feature', data=importance_df.head(top_n), hue='Feature', palette='viridis')\n","        plt.title(f\"Feature Importances: {name} ({feature_group})\")\n","        plt.xlabel(\"Importance Score\")\n","        plt.ylabel(\"Feature\")\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(results_dir), bbox_inches='tight')\n","        plt.show()\n","\n","\n","    else:\n","        print(f\"{name} does not support feature importance or coefficient extraction.\")\n","\n","    return importance_df"]},{"cell_type":"markdown","source":["#### Functions to save results"],"metadata":{"id":"Zhbcm5R-pD09"}},{"cell_type":"code","source":["# function to save results from classification pipeline\n","def save_results_to_csv(results, label, feature_group, results_dir=results_dir):\n","\n","    for name, result_dict in results.items():\n","        filename = f\"{name}_{label}_{feature_group}.csv\"\n","        filepath = os.path.join(results_dir, 'classification_results', filename)\n","        df = pd.DataFrame([result_dict])\n","        df.to_csv(filepath, index=False)\n","        print(f\"Saved: {filepath}\")\n","\n","\n","# function to merge all results in a single csv file\n","def merge_all_results_to_csv(classifiers, label, feature_groups, results_dir=results_dir):\n","    c = 0\n","    cr_all_results = None\n","    sufix = ''\n","\n","    for name in classifiers:\n","        for feature_group in feature_groups:\n","            results_path = os.path.join(results_dir, 'classification_results', f\"{name}_{label}_{feature_group}.csv\")\n","            if not os.path.exists(results_path):\n","                print(f\"Attempted to merge: {results_path} not found\")\n","                continue\n","\n","            df_cr = pd.read_csv(results_path)\n","\n","            if c == 0:\n","                cr_all_results = df_cr\n","                c += 1\n","            else:\n","                cr_all_results = pd.concat([cr_all_results, df_cr], axis=0)\n","\n","    if cr_all_results is not None:\n","        cr_filename = f\"classification_results_all{sufix}_{label}.csv\"\n","\n","        cr_all_results.to_csv(os.path.join(results_dir, cr_filename), index=False)\n","    else:\n","        print(f\"No results to merge for {name}\")\n","\n","    print(f\"Merged all result of {c} classifiers to CSV\")"],"metadata":{"id":"10uq07vhpJkq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Hyperparameter Tuning & Evaluation"],"metadata":{"id":"sc9hCrEOpTz5"}},{"cell_type":"code","source":["def evaluate_pipeline(pipelines, param_grids, df, cols, label, feature_group, results_dir=results_dir):\n","\n","    X_train, X_test, y_train, y_test, le = get_training_data(df, cols, label) # get training and test data\n","\n","    best_models = {}\n","    save_results = {}\n","\n","    print(pipelines)\n","\n","    for name in pipelines.keys():\n","\n","        # initialise results dictionary\n","        save_results[name] = {\"Classifier\": name, \"Feature set\": feature_group,\n","                              \"Accuracy (5-fold CV) - Mean\": 0.0, \"Accuracy (5-fold CV) - Std Dev\": 0.0, \"Test Accuracy\": 0.0,\n","                              \"Best Parameters\": None, \"All Parameters\": None, \"Classification Report\": None, \"Feature Importances\": None}\n","\n","        # calling GridSearch with 5-foldCV to look for optimal set of hyperparameters using the previously defined parameter grid and pipelines\n","        grid = GridSearchCV(pipelines[name], param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)\n","        grid.fit(X_train, y_train)\n","\n","        mean_accuracies = grid.cv_results_['mean_test_score']  # calculate average accuracy across all 5-folds\n","        std_accuracies = grid.cv_results_['std_test_score']  # calculate std of accuracy across all 5-folds\n","\n","        print(\"Accuracy statistics across 5-fold cross-validation:\")\n","\n","        all_params = \"\"\n","        best_acc = 0.0\n","        best_std = 0.0\n","\n","        for params, mean_acc, std_acc in zip(grid.cv_results_['params'], mean_accuracies, std_accuracies): # iterate over classification results and accuracy results simultaneously\n","           print(f\"{params}: Mean Accuracy = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\")\n","           all_params += f\"{params}: Mean Accuracy = {mean_acc:.4f}, Std Dev = {std_acc:.4f}\\n\"\n","           if mean_acc > best_acc:\n","               best_acc = mean_acc\n","               best_std = std_acc\n","\n","        best_models[name] = grid.best_estimator_\n","\n","        save_results[name][\"Accuracy (5-fold CV) - Mean\"] = f\"{best_acc:.4f}\"\n","        save_results[name][\"Accuracy (5-fold CV) - Std Dev\"] = f\"{best_std:.4f}\"\n","        save_results[name][\"Best Parameters\"] = grid.best_estimator_\n","        save_results[name][\"All Parameters\"] = all_params\n","\n","        # evaluate optimised model on the test set\n","        y_pred = grid.predict(X_test)\n","        acc = accuracy_score(y_test, y_pred) # obtain accuracy scores on test set\n","\n","        print(f\"Best Parameters for {name}: {grid.best_params_}\")\n","        print(\"Test Accuracy: {:.2f}%\".format(acc * 100))\n","\n","        save_results[name][\"Test Accuracy\"] = f\"{acc:.4f}\"\n","\n","        print(\"Classification Report:\")\n","        c_report = classification_report(y_test, y_pred, target_names=le.classes_)\n","        print(c_report)\n","        print(\"-\" * 50)\n","\n","        save_results[name][\"Classification Report\"] = c_report\n","\n","        # plot confusion matrix\n","        plot_confusion_matrix(name, label, le, y_test, y_pred, feature_group, results_dir=results_dir)\n","\n","        # plot feature importances\n","        clf = grid.best_estimator_.named_steps['clf'] # get classifier\n","        feature_names = X_train.columns\n","        importances = plot_features_importance(name, clf, label, feature_names, feature_group, results_dir=results_dir)\n","        print(\"=\" * 80)\n","\n","        if importances is not None:\n","            save_results[name][\"Feature Importances\"] = importances.head(10)\n","        else:\n","            save_results[name][\"Feature Importances\"] = \"Not supported for this classifier\"\n","\n","        # save results to csv file\n","        save_results_to_csv(save_results, label, feature_group, results_dir=results_dir)"],"metadata":{"id":"_VShz1Gopa3r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6fkpxD5fVjSS"},"source":["#### Model Training & Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEnLHZ0DVjSS"},"outputs":[],"source":["# uncomment to select dataset / label to use\n","\n","FRAME_LEVEL = True # True for frame-level, False for video-level\n","LABEL = 'emotion'\n","#LABEL = 'sentiment'\n","\n","# uncomment to select the different subset of features (testing different modalities on classificaiton performance)\n","f_group = feature_group # 'rgb_hsv','audio', 'optical_flow'\n","'''\n","f_group = ['rgb_hsv', 'audio']\n","#Â f_group = ['optical_flow']\n","# merged_feature_group = join_feature_datasets(['rgb_hsv', 'optical_flow'])\n","# f_group = [merged_feature_group]\n","'''\n","\n","for FEATURE_GROUP in f_group:\n","\n","    frame_df, video_df, feature_cols = get_feature_group_datasets(FEATURE_GROUP)\n","\n","    if FRAME_LEVEL:\n","        df = frame_df\n","        dataset = \"frame_level\"\n","    else:\n","        df = video_df\n","        dataset = \"video_level\"\n","\n","    # Evaluate all pipelines\n","    print(\"=== EVALUATING PIPELINES ===\\n\")\n","    evaluate_pipeline(pipelines, param_grids, df, feature_cols, LABEL, FEATURE_GROUP, results_dir=os.path.join(results_dir, dataset, \"modelling\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zywfJVVcVjST"},"outputs":[],"source":["merge_all_results_to_csv(pipelines.keys(), LABEL, feature_group, results_dir=os.path.join(results_dir, dataset, \"modelling\"))"]}],"metadata":{"colab":{"collapsed_sections":["pRqUq8AvG4wi"],"provenance":[{"file_id":"1TO6eLQkUwGtYb9yb5ycBytPGMh5QzkRG","timestamp":1745659251001}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}