{"cells":[{"cell_type":"markdown","metadata":{"id":"Y9lOpSPnBGIY"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7l1ZVgY3Cxwg"},"outputs":[],"source":["from my_utils import emotion_dict, sentiment_dict, film_frames_dict\n","\n","import os\n","from tqdm import tqdm\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import librosa\n","import librosa.display"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PY9lni8jD9U8"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","folder = '/content/drive/My Drive/Colab Notebooks/Dissertation'\n","\n","os.chdir(folder)\n","\n","video_dir = '../../Cropped_Videos' # folder where film clips are stored\n","audio_dir = '../../Audio' # folder where film audio files are stored\n","output_dir = './Datasets'\n","results_dir = './Results'\n","\n","# create folders for saving results\n","os.makedirs(output_dir, exist_ok=True)\n","os.makedirs(results_dir, exist_ok=True)\n","\n","film_keys = list(emotion_dict.keys())\n","datasets = ['frame_level', 'video_level']\n","results = ['eda', 'stats_analysis','modelling']\n","feature_group = ['rgb_hsv','audio','optical_flow']\n","\n","# creating folders to save feature sets and results for Data Analysis / Modelling for each frame_level and video_level\n","for dataset in datasets:\n","    os.makedirs(os.path.join(output_dir, dataset), exist_ok=True)\n","    os.makedirs(os.path.join(results_dir, dataset), exist_ok=True)\n","    for r_type in results:\n","        os.makedirs(os.path.join(results_dir, dataset, r_type), exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"DZNoyRDPBGIc"},"source":["# 1. Feature Extraction"]},{"cell_type":"markdown","metadata":{"id":"iT483LUjBGIc"},"source":["#### Function to merge features from all the videos in a single file per video id (for both frame-level features and video-level features)\n","\n","* *Frame-level features*: result in 1 row of features per image frame\n","* *Video-level features*: result in 1 row of features per video, by averaging the values of all frame-level feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OY1MFkPsBGIc"},"outputs":[],"source":["# Function to merge all features from all videos in the dataset for a given feature group\n","\n","def merge_dataset(input_dir, feature_group):\n","\n","    output_file = os.path.join(input_dir, f\"features_{feature_group}_df.csv\") # file name for saving the dataframe with all extracted features\n","    files = [f for f in os.listdir(input_dir) if f.endswith(f\"_{feature_group}_df.csv\")]\n","    merged_df = pd.concat([pd.read_csv(os.path.join(input_dir, f)) for f in files], ignore_index=True)\n","    merged_df = merged_df.sort_values(by=\"video_id\")\n","    sort_columns = [\"video_id\", \"frame_id\"] if merged_df.columns[1] == \"frame_id\" else [\"video_id\"]\n","    merged_df = merged_df.sort_values(by=sort_columns, ascending=True)\n","    merged_df.to_csv(output_file, index=False, mode='w')\n","    print(f\"Merged {feature_group} features saved to {output_file}\")\n"]},{"cell_type":"markdown","metadata":{"id":"ZE4NS195BGId"},"source":["### 1.1 RGB_HSV features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAD0GqG0BGId"},"outputs":[],"source":["def extract_rgb_hsv_statistics(frame):\n","    # function takes in an RGB image frame\n","    rgb = cv2.split(frame) # get the RGB values\n","    hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV) # convert to HSV\n","    hsv = cv2.split(hsv) # get the HSV values\n","\n","    # Compute mean, median, std for each channel across all the pixels in the frame\n","    features = {}\n","    channels = ['R', 'G', 'B', 'H', 'S', 'V']\n","    for i, channel in enumerate(rgb + hsv):\n","        features[f\"{channels[i]}_mean\"] = np.mean(channel)\n","        features[f\"{channels[i]}_median\"] = np.median(channel)\n","        features[f\"{channels[i]}_std\"] = np.std(channel)\n","\n","    return features\n","\n","def extract_colourfulness(frame):\n","    (R, G, B) = cv2.split(frame.astype(\"float\"))\n","    rg = np.abs(R - G)\n","    yb = np.abs(0.5 * (R + G) - B)\n","\n","    var_rg, mu_rg = np.var(rg), np.mean(rg)\n","    var_yb, mu_yb = np.var(yb), np.mean(yb)\n","\n","    mu_sq = (mu_rg ** 2) + (mu_yb ** 2)\n","\n","    colourfulness = np.sqrt(var_rg + var_yb) + 0.3 * np.sqrt(mu_sq)\n","\n","    return colourfulness\n","\n","\n","def extract_rgb_hsv_features(video_path, frame_interval=0, output_dir=output_dir):\n","\n","    frame_idx = 0\n","    f_idx = 0\n","    features_list = []\n","\n","    cap = cv2.VideoCapture(video_path)\n","    # obtain the video id based on the file name\n","    video_id = int(os.path.splitext(os.path.basename(video_path))[0].split('_')[0])\n","\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))  # get number of frames per second\n","    frame_interval = round(fps)\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # capture only one frame per second\n","        if frame_interval <= 0 or frame_idx % frame_interval == 0:\n","\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","            rgb_hsv = extract_rgb_hsv_statistics(frame)\n","            colourfulness = extract_colourfulness(frame)\n","            rgb_hsv['colourfulness'] = colourfulness\n","\n","            # append video id, frame id, emotion, sentiment\n","            frame_level = {\"video_id\": video_id, \"frame_id\": f_idx,\n","                           \"emotion\": emotion_dict[video_id],\n","                           \"sentiment\": sentiment_dict[video_id]}\n","            frame_level.update(rgb_hsv)\n","            features_list.append(frame_level)\n","\n","            f_idx += 1\n","\n","        frame_idx += 1\n","\n","    cap.release()\n","\n","    # save all frame features into a single CSV file per video\n","    video_df = pd.DataFrame(features_list)\n","\n","    video_filename = os.path.join(output_dir, f\"frame_level/{video_id}_frames_rgb_hsv_df.csv\")\n","    video_df.to_csv(video_filename, index=False)\n","\n","    # save video-level features\n","    video_level = video_df.drop(columns=[\"frame_id\"]).groupby([\"video_id\",\"emotion\",\"sentiment\"]).mean().reset_index() # take the mean of all the features for each video\n","    video_level_filename = os.path.join(output_dir, f\"video_level/{video_id}_video_rgb_hsv_df.csv\")\n","    video_level.to_csv(video_level_filename, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAp6YWx5BGIe"},"outputs":[],"source":["# run to extract rgb_hsv features from videos\n","# tqdm used to output a progress bar given that this process was computationally expensive, and was taking a long time to complete\n","\n","for vid in tqdm(film_keys, desc=\"Processing videos\"):\n","    for ext in [\".mp4\", \".mpeg\", \".mpg\", \".mov\"]: # check the file format\n","        video_path = os.path.join(video_dir, f\"{vid}_cropped{ext}\")\n","        if os.path.exists(video_path):\n","            extract_rgb_hsv_features(video_path)\n","\n","print(\"Completed colour feature extraction for all videos\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWmjm0IHBGIf"},"outputs":[],"source":["# save rgb_hsv features from all videos into a single file, both at frame-level & video-level\n","input_dir = os.path.join(output_dir, \"frame_level\")\n","merge_dataset(input_dir, 'rgb_hsv')\n","\n","input_dir = os.path.join(output_dir, \"video_level\")\n","merge_dataset(input_dir, 'rgb_hsv')"]},{"cell_type":"markdown","metadata":{"id":"ZYujIDeXBGIf"},"source":["### 1.2 Audio features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lloRU6BeBGIf"},"outputs":[],"source":["def extract_audio_features(audio_path, output_dir=output_dir):\n","\n","    # obtain the audio id based on the file name\n","    audio_id = int(os.path.splitext(os.path.basename(audio_path))[0].split('_')[0].split('_')[0])\n","\n","    # load audio file\n","    signal, sampling_rate = librosa.load(audio_path)\n","\n","    print(\"Sampling rate:\", sampling_rate)\n","\n","    # values ensure that one-second segments of audio are extracted\n","    FRAME_SIZE = np.round(sampling_rate)\n","    HOP_LENGTH = np.round(sampling_rate)\n","\n","    # extract audio features from each one-second segment\n","    spectral_centroid = librosa.feature.spectral_centroid(y=signal, sr=sampling_rate, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH)\n","    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=signal, sr=sampling_rate, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH)\n","    rms_energy = librosa.feature.rms(y=signal, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)\n","    zero_crossing_rate = librosa.feature.zero_crossing_rate(y=signal, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)\n","    chroma = librosa.feature.chroma_stft(y=signal, hop_length=HOP_LENGTH)\n","    chroma_mean = np.mean(chroma, axis=0).reshape(1, -1)\n","    mfccs = librosa.feature.mfcc(y=signal, n_mfcc=13, sr=sampling_rate, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH)\n","    mfccs_mean = np.mean(mfccs, axis=0).reshape(1, -1)\n","    delta_mfccs = librosa.feature.delta(mfccs)\n","    delta_mfccs_mean = np.mean(delta_mfccs, axis=0).reshape(1, -1)\n","    delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n","    delta2_mfccs_mean = np.mean(delta2_mfccs, axis=0).reshape(1, -1)\n","\n","    # calculate amplitude envelope for each one-second segment\n","    amplitude_envelope = []\n","\n","    for i in range (0, len(signal), HOP_LENGTH):\n","        curr_frame_amplitude_envelope = max(signal[i:i+FRAME_SIZE])\n","        amplitude_envelope.append(curr_frame_amplitude_envelope)\n","\n","    amplitude_envelope = np.array(amplitude_envelope).reshape(1, -1)\n","\n","    # ensure consistent feature length\n","    min_length = min(spectral_centroid.shape[1], spectral_bandwidth.shape[1], \\\n","                    rms_energy.shape[1], zero_crossing_rate.shape[1], \\\n","                    amplitude_envelope.shape[1], mfccs_mean.shape[1], chroma_mean.shape[1])\n","\n","    # Truncate all features to the same length\n","    spectral_centroid = spectral_centroid[:, :min_length]\n","    spectral_bandwidth = spectral_bandwidth[:, :min_length]\n","    rms_energy = rms_energy[:, :min_length]\n","    zero_crossing_rate = zero_crossing_rate[:, :min_length]\n","    chroma = chroma[:, :min_length]\n","    chroma_mean = chroma_mean[:, :min_length]\n","    mfccs = mfccs[:, :min_length]\n","    mfccs_mean = mfccs_mean[:, :min_length]\n","    delta_mfccs = delta_mfccs[:, :min_length]\n","    delta_mfccs_mean = delta_mfccs_mean[:, :min_length]\n","    delta2_mfccs = delta2_mfccs[:, :min_length]\n","    delta2_mfccs_mean = delta2_mfccs_mean[:, :min_length]\n","    amplitude_envelope = amplitude_envelope[:, :min_length]\n","\n","    # stack features into a final feature vector for each sample\n","    features = np.vstack([spectral_centroid, spectral_bandwidth, rms_energy,\n","                          zero_crossing_rate, amplitude_envelope, chroma_mean,\n","                          mfccs_mean, delta_mfccs_mean, delta2_mfccs_mean,\n","                          chroma, mfccs, delta_mfccs, delta2_mfccs]).T\n","\n","    num_feature_vector = features.shape[0]  # calculating the total number of feature vectors\n","\n","    # define feature names\n","    feature_names = [\"spectral_centroid\", \"spectral_bandwidth\", \"rms_energy\",\n","                     \"zero_crossing_rate\", \"amplitude_envelope\",\"chroma_mean\",\n","                     \"mfccs_mean\", \"delta_mfccs_mean\", \"delta2_mfccs_mean\"]\n","\n","    for i in range(0,chroma.shape[0]):\n","        feature_names.append(f\"chroma_{i}\")\n","    for i in range(0,mfccs.shape[0]):\n","        feature_names.append(f\"mfccs_{i}\")\n","    for i in range(0,delta_mfccs.shape[0]):\n","        feature_names.append(f\"delta_mfccs_{i}\")\n","    for i in range(0,delta2_mfccs.shape[0]):\n","        feature_names.append(f\"delta2_mfccs_{i}\")\n","\n","    # creating a data frame for audio features\n","    video_df = pd.DataFrame(features, columns=feature_names)\n","\n","    video_id = audio_id\n","    f_idxs = np.arange(num_feature_vector)\n","\n","    features_list = {\"video_id\": audio_id,\n","                     \"frame_id\": f_idxs,\n","                     \"emotion\": emotion_dict[audio_id],\n","                     \"sentiment\": sentiment_dict[audio_id] }\n","\n","    video_df = pd.concat([pd.DataFrame(features_list), video_df], axis=1)\n","\n","    video_id = int(os.path.splitext(os.path.basename(audio_path))[0].split('_')[0].split('_')[0])\n","    video_filename = os.path.join(output_dir, f\"frame_level/{video_id}_frames_audio_df.csv\")\n","    video_df.to_csv(video_filename, index=False)\n","\n","    # Save video-level summary\n","    video_level = video_df.drop(columns=[\"frame_id\"]).groupby([\"video_id\",\"emotion\",\"sentiment\"]).mean().reset_index() # take the mean of all the features for each video\n","    video_level_filename = os.path.join(output_dir, f\"video_level/{video_id}_video_audio_df.csv\")\n","    video_level.to_csv(video_level_filename, index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CePQ6tVBGIg"},"outputs":[],"source":["# run to extract audio features from videos\n","for vid in tqdm(film_keys, desc=\"Processing audio\"):\n","    audio_path = os.path.join(audio_dir, f\"{vid}_audio.mp3\")\n","    if os.path.exists(audio_path):\n","        extract_audio_features(audio_path)\n","\n","# merge all audio features into a single file\n","input_dir = os.path.join(output_dir, \"video_level\")\n","merge_dataset(input_dir, 'audio')\n","\n","# merge all audio features into a single file\n","input_dir = os.path.join(output_dir, \"frame_level\")\n","merge_dataset(input_dir, 'audio')\n","\n","print(\"Completed audio feature extraction for all videos\")"]},{"cell_type":"markdown","metadata":{"id":"iyQDD28-BGIg"},"source":["### 1.3 Motion (Optical Flow) features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luIlHVjwBGIh"},"outputs":[],"source":["# extract optical flow features using Farneback's method\n","\n","def extract_optical_flow_features(video_path, frame_interval=0, output_dir=output_dir):\n","\n","    frame_idx = 0\n","    f_idx = 0\n","    features_list = []\n","\n","    cap = cv2.VideoCapture(video_path)\n","    video_id = int(os.path.splitext(os.path.basename(video_path))[0].split('_')[0])\n","\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))  # get frames per second\n","    frame_interval = round(fps)\n","\n","    # read the first frame\n","    ret, prev_frame = cap.read()\n","    if not ret:\n","        print(f\"Failed to read the video: {video_path}\")\n","        return\n","\n","    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n","\n","    while cap.isOpened():\n","\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        if frame_interval <= 0 or frame_idx % frame_interval == 0: # ensures that frames are being compared at 1 second intervals\n","\n","            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","            # Calculate dense optical flow\n","            flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None,\n","                                                pyr_scale=0.5, levels=3, winsize=15,\n","                                                iterations=3, poly_n=5, poly_sigma=1.2, flags=0)\n","\n","            # Compute magnitude and angle of the flow\n","            magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n","\n","            # Average magnitude and angle\n","            avg_magnitude = np.mean(magnitude)\n","            avg_angle = np.mean(angle)\n","\n","            frame_level = { 'video_id': video_id,\n","                           'frame_id': f_idx, \"emotion\": emotion_dict[video_id],\n","                            \"sentiment\": sentiment_dict[video_id],\n","                            'avg_magnitude': avg_magnitude, 'avg_angle': avg_angle }\n","\n","            prev_gray = gray # set previous frame as the current frame\n","            f_idx += 1\n","\n","            features_list.append(frame_level)\n","\n","        frame_idx += 1\n","\n","    cap.release()\n","\n","    # save features to a CSV file\n","    df = pd.DataFrame(features_list)\n","\n","    # save frame-level features\n","    os.makedirs(os.path.join(output_dir, \"frame_level\"), exist_ok=True)\n","    output_file = os.path.join(output_dir, f\"frame_level/{video_id}_optical_flow_df.csv\")\n","    df.to_csv(output_file, index=False)\n","\n","    # save video-level features\n","    video_summary = df.drop(columns=[\"frame_id\"]).groupby([\"video_id\", \"emotion\", \"sentiment\"]).mean().reset_index() # take the mean of all the features for each video\n","    os.makedirs(os.path.join(output_dir, \"video_level\"), exist_ok=True)\n","    summary_file = os.path.join(output_dir, f\"video_level/{video_id}_optical_flow_df.csv\")\n","    video_summary.to_csv(summary_file, index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBc7dyIBBGIh"},"outputs":[],"source":["# run to extract optical flow features\n","\n","for vid in tqdm(film_keys, desc=\"Processing videos\"):\n","    for ext in [\".mp4\", \".mpeg\", \".mpg\", \".mov\"]:\n","        video_path = os.path.join(video_dir, f\"{vid}_cropped{ext}\")\n","        if os.path.exists(video_path):\n","            extract_optical_flow_features(video_path)\n","\n","# merge all optical flow features into a single file\n","input_dir = os.path.join(output_dir, \"frame_level\")\n","merge_dataset(input_dir, 'optical_flow')\n","\n","# merge all optical flow features into a single file\n","input_dir = os.path.join(output_dir, \"video_level\")\n","merge_dataset(input_dir, 'optical_flow')\n","\n","print(\"Completed optical flow feature extraction for all videos\")"]}],"metadata":{"colab":{"collapsed_sections":["pRqUq8AvG4wi"],"provenance":[{"file_id":"1uKgMH8tsik9pCy9K1Tz3Oqf9wKo7yt7E","timestamp":1745659152681}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}
